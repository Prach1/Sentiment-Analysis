{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assign9a.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prach1/CS69002_9A_18CS60R30/blob/master/DL_Assign9a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WWVo8Rx4IJri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TmKKOcUJIaV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S5R4nGhQGdmT",
        "colab_type": "code",
        "outputId": "8dbe290d-02f0-4f79-dbba-d6984220e22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mEzZuPgfMpjC",
        "colab_type": "code",
        "outputId": "ffe7ea48-3337-4bd4-dcea-5687d77b9f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/Train_20K.csv', sep='\\t')\n",
        "print(df.head())"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  label\n",
            "0  John Waters has given us a genuinely enjoyable...      1\n",
            "1  This first two seasons of this comedy series w...      1\n",
            "2  What an unfortunate mess is \"Shiner.\" I wanted...      0\n",
            "3  I'm not entirely sure Rob Schmidt qualifies as...      1\n",
            "4  i wasn't sure whether to laugh or cry. Porrett...      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OgkNiKQHP7zY",
        "colab_type": "code",
        "outputId": "e77b92f4-edd3-4a4f-8ad1-e5980790a8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/Test_5K.csv', sep='\\t')\n",
        "print(df1.head())"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  label\n",
            "0  as always this is an inaccurate picture of the...      0\n",
            "1  Did the movie-makers even preview this before ...      0\n",
            "2  Heavily re-edited and often confusing, the ori...      0\n",
            "3  I notice that most of the people who think thi...      0\n",
            "4  First of all, this is a low-budget movie, so m...      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fUNCUbMVNmed",
        "colab_type": "code",
        "outputId": "277a4448-73bb-4f4b-f982-08441593fc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "for chunk in df:\n",
        "  print(chunk)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text\n",
            "label\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XNFMU4oAPObi",
        "colab_type": "code",
        "outputId": "8641cb09-282a-46b7-a966-fd981f90ecdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# for i, j in df.iterrows(): \n",
        "#     print(i)\n",
        "#     print(j)\n",
        "\n",
        "print(type(df['text']))\n",
        "print(type(df['label']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKNDD3sbP-ou",
        "colab_type": "code",
        "outputId": "6d37ee30-4d78-4ade-b614-690637034819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text_reviews = df['text'].astype(str).tolist()\n",
        "text_labels = df['label'].astype(int)\n",
        "print(type(text_labels))\n",
        "\n",
        "text_reviews = [x.lower() for x in text_reviews]\n",
        "#print(type(text_reviews))\n",
        "#print(text_reviews[0])"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mV2qpcyVQGPc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_test_reviews = df1['text'].astype(str).tolist()\n",
        "text_test_labels = df1['label'].astype(int)\n",
        "#print(type(text_labels))\n",
        "\n",
        "text_test_reviews = [x.lower() for x in text_test_reviews]\n",
        "#print(type(text_reviews))\n",
        "#print(text_reviews[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5YSFe6HCSDIc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess(text):    \n",
        "  text = re.sub(r'<br.*?>',' ',text)\n",
        "  text = re.sub(r'[^\\w\\s]',' ',text) \n",
        "  text = re.sub(r'[0-9]+', ' ', text)\n",
        "  \n",
        "  return text\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EasTRcAhRolZ",
        "colab_type": "code",
        "outputId": "6910f461-6402-49ab-cc8d-6d9593dc26b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "train_data = []\n",
        "train_data_X = []\n",
        "stop = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for review in text_reviews:\n",
        "  new_review = preprocess(review)\n",
        "  train_data.append(new_review)\n",
        "  \n",
        "print(train_data[0])\n",
        "\n",
        "for review in train_data:\n",
        "  review = [lemmatizer.lemmatize(word) for word in review.split() if word not in stop and word != '' and len(lemmatizer.lemmatize(word)) > 1]\n",
        "  train_data_X.append(review)\n",
        "  \n",
        "print(train_data_X[0])"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "john waters has given us a genuinely enjoyable film  this certainly isn t without its shocking waters esque moments  but it is tamer than his older culty stuff  such as  pink flamingoes    pecker  harkens back to john s early mainstream stage in that it reminds the viewer of the same kind of humor that was evident in  polyester   overall  a really fun comedy with some great moments \n",
            "['john', 'water', 'given', 'genuinely', 'enjoyable', 'film', 'certainly', 'without', 'shocking', 'water', 'esque', 'moment', 'tamer', 'older', 'culty', 'stuff', 'pink', 'flamingo', 'pecker', 'harkens', 'back', 'john', 'early', 'mainstream', 'stage', 'reminds', 'viewer', 'kind', 'humor', 'evident', 'polyester', 'overall', 'really', 'fun', 'comedy', 'great', 'moment']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Zt6iRhMQRxv",
        "colab_type": "code",
        "outputId": "0942db54-9526-4dcd-f11f-dea92306190f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "test_data = []\n",
        "test_data_X = []\n",
        "stop = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for review in text_test_reviews:\n",
        "  new_review = preprocess(review)\n",
        "  test_data.append(new_review)\n",
        "  \n",
        "print(test_data[0])\n",
        "\n",
        "for review in test_data:\n",
        "  review = [lemmatizer.lemmatize(word) for word in review.split() if word not in stop and word != '' and len(lemmatizer.lemmatize(word)) > 1]\n",
        "  test_data_X.append(review)\n",
        "  \n",
        "print(test_data_X[0])"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "as always this is an inaccurate picture of the homeless  tv told a lot of lies about panhandlers in the early  s and made everyone look bad  and claimed we all made over    a day when      a day was much closer to reality  when someone drove by where i held up a sign offering to work  and offered me work  i actually went and took the work if i was physically able and if i would been offered the      id damned sure invested in in apt prepaid for at least   years  and kept most in the bank and still left myself      for nl      and      cash games at the casinos  i usually always win and could win decent if i just had a bankroll  instead i win about    a month is all playing in always minimum buying in due to not wanting to risk losing it all  i was only homeless cause i didn t wanna risk spending all my money and going broke  sometimes i had over      in my sock while i slept outside  anyone wanting to talk contact sevencard  on yahoo messenger i admit i was different than most homeless people though  due to the fact i never drank smoke or took drugs  im no longer homeless  am now in govt housing for    a month and getting ssi and spend most of my time winning at online poker  mom and sunflower diversified worked hard to get me ssi  glad my days of hiding in under the stage in the convention center of the casino at night sleeping  worrying about getting caught by security are finally over  had this tv crew picked me theyd been over a lot sooner  its a shame how they don t better select who they pick \n",
            "['always', 'inaccurate', 'picture', 'homeless', 'tv', 'told', 'lot', 'lie', 'panhandler', 'early', 'made', 'everyone', 'look', 'bad', 'claimed', 'made', 'day', 'day', 'much', 'closer', 'reality', 'someone', 'drove', 'held', 'sign', 'offering', 'work', 'offered', 'work', 'actually', 'went', 'took', 'work', 'physically', 'able', 'would', 'offered', 'id', 'damned', 'sure', 'invested', 'apt', 'prepaid', 'least', 'year', 'kept', 'bank', 'still', 'left', 'nl', 'cash', 'game', 'casino', 'usually', 'always', 'win', 'could', 'win', 'decent', 'bankroll', 'instead', 'win', 'month', 'playing', 'always', 'minimum', 'buying', 'due', 'wanting', 'risk', 'losing', 'homeless', 'cause', 'wanna', 'risk', 'spending', 'money', 'going', 'broke', 'sometimes', 'sock', 'slept', 'outside', 'anyone', 'wanting', 'talk', 'contact', 'sevencard', 'yahoo', 'messenger', 'admit', 'different', 'homeless', 'people', 'though', 'due', 'fact', 'never', 'drank', 'smoke', 'took', 'drug', 'im', 'longer', 'homeless', 'govt', 'housing', 'month', 'getting', 'ssi', 'spend', 'time', 'winning', 'online', 'poker', 'mom', 'sunflower', 'diversified', 'worked', 'hard', 'get', 'ssi', 'glad', 'day', 'hiding', 'stage', 'convention', 'center', 'casino', 'night', 'sleeping', 'worrying', 'getting', 'caught', 'security', 'finally', 'tv', 'crew', 'picked', 'theyd', 'lot', 'sooner', 'shame', 'better', 'select', 'pick']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-h1LFhDrcFgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_word_frequency(dataset):\n",
        "  word_to_frequency = {}\n",
        " \n",
        "  for sent in dataset:\n",
        "    for word in sent:\n",
        "      if word not in word_to_frequency:\n",
        "        word_to_frequency[word] = 1\n",
        "      else:\n",
        "        word_to_frequency[word] += 1\n",
        "              \n",
        "  return word_to_frequency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gP8ClEwsGRHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_bigram_word_frequency(dataset):\n",
        "  word_to_frequency = {}\n",
        "  \n",
        "  for sent in dataset:\n",
        "    for index,word1 in enumerate(sent[:-1]):\n",
        "      word2 = sent[index+1]\n",
        "      if (word1,word2) not in word_to_frequency:\n",
        "        word_to_frequency[(word1,word2)] = 1\n",
        "      else:\n",
        "        word_to_frequency[(word1,word2)] += 1\n",
        "              \n",
        "  return word_to_frequency  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jT64fOuh1bRE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_word_ids(dataset):\n",
        "  word_to_ix = {}\n",
        " \n",
        "  for word in dataset:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "              \n",
        "  word_to_ix['UNKNOWN'] = len(word_to_ix) \n",
        "              \n",
        "  return word_to_ix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDk2HppkLNmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_bigram_word_ids(dataset):\n",
        "  word_to_ix = {}\n",
        " \n",
        "  for word in dataset:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "              \n",
        "  word_to_ix['UNKNOWN'] = len(word_to_ix) \n",
        "              \n",
        "  return word_to_ix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBuHYkyVbwj_",
        "colab_type": "code",
        "outputId": "8934cdab-1927-40e3-f767-b0e2e39d3051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "word_to_frequency = generate_word_frequency(train_data_X + test_data_X)\n",
        "# for key in word_to_bigram_frequency:\n",
        "#   print(key)\n",
        "#   break\n",
        "\n",
        "new_word_dataset = []\n",
        "\n",
        "for word in word_to_frequency:\n",
        "  if(word_to_frequency[word] >= 3):\n",
        "    new_word_dataset.append(word)\n",
        "        \n",
        "print(len(word_to_frequency))\n",
        "print(len(new_word_dataset))\n",
        "\n",
        "word_to_ix = generate_word_ids(new_word_dataset)\n",
        "\n",
        "print(len(word_to_ix))\n",
        "print(word_to_ix['UNKNOWN'])\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = 2"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63726\n",
            "32047\n",
            "32048\n",
            "32047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9J59Wdd_Ypjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "61d2f142-6b67-4f08-a24e-f72833505484"
      },
      "cell_type": "code",
      "source": [
        "word_to_bigram_frequency = generate_bigram_word_frequency(train_data_X + test_data_X)\n",
        "new_bigram_word_dataset = []\n",
        "\n",
        "for word in word_to_bigram_frequency:\n",
        "  if(word_to_bigram_frequency[word] >= 7):\n",
        "    new_bigram_word_dataset.append(word)\n",
        "    \n",
        "print(len(word_to_bigram_frequency))\n",
        "print(len(new_bigram_word_dataset))\n",
        "#print(new_bigram_word_dataset[0])\n",
        "\n",
        "word_to_bigram_ix = generate_bigram_word_ids(new_bigram_word_dataset)\n",
        "BIGRAM_VOCAB_SIZE = len(word_to_bigram_ix)\n",
        "print(BIGRAM_VOCAB_SIZE)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1554114\n",
            "39242\n",
            "39243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kCBikQgFeNzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bow_vector(sentence):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            #raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            vec[word_to_ix['UNKNOWN']]+=1\n",
        "        else:\n",
        "            vec[word_to_ix[word]]+=1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "# def make_target(label, label_to_ix):\n",
        "#     return torch.LongTensor([label_to_ix[label]])\n",
        "    #return torch.LongTensor(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVol8s9ZOe1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bigram_bow_vector(sentence):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_bigram_ix))\n",
        "    for index,word1 in enumerate(sentence[:-1]):\n",
        "      word2 = sentence[index+1]\n",
        "      if (word1,word2) not in word_to_bigram_ix:\n",
        "        vec[word_to_bigram_ix['UNKNOWN']]+=1\n",
        "      else:\n",
        "        vec[word_to_bigram_ix[(word1,word2)]]+=1\n",
        "    return vec.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eVHXo-z6o5Ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = []\n",
        "\n",
        "for review in train_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bow_vector(review)).cuda()\n",
        "  train.append(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4daeTqFXOisC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bigram_train = []\n",
        "\n",
        "for review in train_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bigram_bow_vector(review)).cuda()\n",
        "  bigram_train.append(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lO7x_vpokwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = []\n",
        "for review in test_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bow_vector(review)).cuda()\n",
        "  test.append(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLGnoEvOOlsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bigram_test = []\n",
        "for review in test_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bigram_bow_vector(review)).cuda()\n",
        "  bigram_test.append(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q629yqi92ICc",
        "colab_type": "code",
        "outputId": "419da647-9efa-4f78-888a-581042a674f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(train[0])\n",
        "print(len(train))\n",
        "print(test[4])\n",
        "print(len(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 1.,  ..., 0., 0., 1.]], device='cuda:0')\n",
            "17999\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uUMS1uUm5agi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data = []\n",
        "\n",
        "# for index,review in enumerate(train):\n",
        "#   data.append((review,text_labels[index]))\n",
        "  \n",
        "# print(data[0])\n",
        "\n",
        "#label_to_tensor = torch.LongTensor(text_labels)\n",
        "\n",
        "label_to_tensor = []\n",
        "\n",
        "#print(text_labels)\n",
        "text_labels1 = []\n",
        "\n",
        "for label in text_labels:\n",
        "  text_labels1.append([label])\n",
        "  \n",
        "\n",
        "#print(text_labels1)\n",
        "\n",
        "for label in text_labels1:\n",
        "  label_to_tensor.append(torch.tensor(label, dtype = torch.long))\n",
        "  #print(label_to_tensor)\n",
        "  \n",
        "#print(label_to_tensor)\n",
        "\n",
        "#for label in label_to_tensor:\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TAlc4HZIuuSM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_to_test_tensor = []\n",
        "\n",
        "text_test_labels1 = []\n",
        "\n",
        "for label in text_test_labels:\n",
        "  text_test_labels1.append([label])\n",
        "\n",
        "for label in text_test_labels1:\n",
        "  label_to_test_tensor.append(torch.tensor(label, dtype = torch.long))\n",
        "  \n",
        "#print(label_to_test_tensor)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLRwTztI8VPb",
        "colab_type": "code",
        "outputId": "0c34fad7-dc6c-42c3-8a71-c2529b797874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.device('cuda') "
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "metadata": {
        "id": "3X4ztERz78r1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task1(nn.Module):\n",
        "  def __init__(self, num_labels,hidden,vocab_size):\n",
        "    super(Task1, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden)\n",
        "    self.lin2 = nn.Linear(hidden, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.softmax(self.lin2(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtj4mNPeuMes",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task2(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,vocab_size):\n",
        "    super(Task2, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    self.lin3 = nn.Linear(hidden2, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = F.softmax(self.lin3(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mYePz-uhxAKJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task3(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,hidden3,vocab_size):\n",
        "    super(Task3, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    self.lin3 = nn.Linear(hidden2, hidden3)\n",
        "    self.lin4 = nn.Linear(hidden3, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    out = F.softmax(self.lin4(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xf1EUT_b1anV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task4(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,hidden3,vocab_size):\n",
        "    super(Task4, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin3 = nn.Linear(hidden2, hidden3)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin4 = nn.Linear(hidden3, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    out = F.softmax(self.lin4(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RaIw3shQU3TN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task6(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,hidden3,vocab_size):\n",
        "    super(Task6, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin3 = nn.Linear(hidden2, hidden3)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin4 = nn.Linear(hidden3, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    out = F.softmax(self.lin4(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o0RdLlbP8Gv9",
        "colab_type": "code",
        "outputId": "a33b9c33-4f55-4e99-9abb-aa1c93d666ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "hidden = 200\n",
        "bow = Task1(NUM_LABELS,hidden,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task1(\n",
            "  (lin1): Linear(in_features=32048, out_features=200, bias=True)\n",
            "  (lin2): Linear(in_features=200, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OM82SJiyu-RY",
        "colab_type": "code",
        "outputId": "9c96a838-99d3-4732-daff-dc0f86911ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 10\n",
        "hidden2 = 10\n",
        "bow = Task2(NUM_LABELS,hidden1,hidden2,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task2(\n",
            "  (lin1): Linear(in_features=32048, out_features=10, bias=True)\n",
            "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (lin3): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GLf_5IEgxVHY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 100\n",
        "hidden2 = 50\n",
        "hidden3 = 10\n",
        "bow = Task3(NUM_LABELS,hidden1,hidden2,hidden3,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QZcU4yR3xzD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 30\n",
        "hidden2 = 20\n",
        "hidden3 = 10\n",
        "bow = Task4(NUM_LABELS,hidden1,hidden2,hidden3,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SONV3i30U9ej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f1ced9fe-c6fc-4df6-c404-11595c88b9d7"
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 30\n",
        "hidden2 = 20\n",
        "hidden3 = 10\n",
        "bow = Task6(NUM_LABELS,hidden1,hidden2,hidden3,BIGRAM_VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task6(\n",
            "  (lin1): Linear(in_features=39243, out_features=30, bias=True)\n",
            "  (lin2): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (lin3): Linear(in_features=20, out_features=10, bias=True)\n",
            "  (lin4): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8E_6XAY8o56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "opt = torch.optim.SGD(bow.parameters(), lr = 0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IyjEzEvo-BIl",
        "colab_type": "code",
        "outputId": "f5a73107-ea4b-4771-e611-1621c5236efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# the training loop\n",
        "for epoch in range(10):\n",
        "  start = time.time()\n",
        "  for index, instance in enumerate(train):\n",
        "    bow.zero_grad()\n",
        "    label = Variable(label_to_tensor[index]).cuda()       \n",
        "    probs = bow(instance).cuda() # forward pass\n",
        "    loss = loss_function(probs, label)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  end = time.time()  \n",
        "  print('epoch is: ' +str(epoch) + ' and loss is: ' +str(loss.data) + ' and time taken is: ' +str(end-start))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch is: 0 and loss is: tensor(-1., device='cuda:0') and time taken is: 37.2442946434021\n",
            "epoch is: 1 and loss is: tensor(-1.0000, device='cuda:0') and time taken is: 36.94411373138428\n",
            "epoch is: 2 and loss is: tensor(-1., device='cuda:0') and time taken is: 36.95587968826294\n",
            "epoch is: 3 and loss is: tensor(-0.9916, device='cuda:0') and time taken is: 37.310444593429565\n",
            "epoch is: 4 and loss is: tensor(-1., device='cuda:0') and time taken is: 37.168681144714355\n",
            "epoch is: 5 and loss is: tensor(-0.9998, device='cuda:0') and time taken is: 37.18535256385803\n",
            "epoch is: 6 and loss is: tensor(-1., device='cuda:0') and time taken is: 37.006630659103394\n",
            "epoch is: 7 and loss is: tensor(-1., device='cuda:0') and time taken is: 36.97114324569702\n",
            "epoch is: 8 and loss is: tensor(-1., device='cuda:0') and time taken is: 36.99241662025452\n",
            "epoch is: 9 and loss is: tensor(-1., device='cuda:0') and time taken is: 36.99219298362732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o_YBrIQcVL1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "44fe96f3-da90-4fb6-c1a3-8a40f41a8f4b"
      },
      "cell_type": "code",
      "source": [
        "#Bigram#\n",
        "\n",
        "import time\n",
        "\n",
        "# the training loop\n",
        "for epoch in range(10):\n",
        "  start = time.time()\n",
        "  for index, instance in enumerate(bigram_train):\n",
        "    bow.zero_grad()\n",
        "    label = Variable(label_to_tensor[index]).cuda()       \n",
        "    probs = bow(instance).cuda() # forward pass\n",
        "    loss = loss_function(probs, label)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  end = time.time()  \n",
        "  print('epoch is: ' +str(epoch) + ' and loss is: ' +str(loss.data) + ' and time taken is: ' +str(end-start))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch is: 0 and loss is: tensor(-5.7215e-14, device='cuda:0') and time taken is: 24.56579089164734\n",
            "epoch is: 1 and loss is: tensor(-5.7111e-14, device='cuda:0') and time taken is: 24.55873394012451\n",
            "epoch is: 2 and loss is: tensor(-8.3977e-14, device='cuda:0') and time taken is: 24.469730854034424\n",
            "epoch is: 3 and loss is: tensor(-1.4182e-13, device='cuda:0') and time taken is: 24.326232194900513\n",
            "epoch is: 4 and loss is: tensor(-2.6186e-13, device='cuda:0') and time taken is: 24.398052215576172\n",
            "epoch is: 5 and loss is: tensor(-7.2919e-13, device='cuda:0') and time taken is: 24.407428979873657\n",
            "epoch is: 6 and loss is: tensor(-5.0875e-14, device='cuda:0') and time taken is: 24.47694230079651\n",
            "epoch is: 7 and loss is: tensor(-2.9990e-16, device='cuda:0') and time taken is: 24.403366804122925\n",
            "epoch is: 8 and loss is: tensor(-5.9114e-15, device='cuda:0') and time taken is: 24.32275629043579\n",
            "epoch is: 9 and loss is: tensor(-0.0271, device='cuda:0') and time taken is: 24.79916000366211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TFTWGa_wre-1",
        "colab_type": "code",
        "outputId": "5cad776c-4335-4262-af9a-0f6d75328ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "cell_type": "code",
      "source": [
        "torch.save(bow,'Task1C.mdl')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('Task1C.mdl')"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Task1. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-171-7b901966eb31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Task1C.mdl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "E1Iyc2KHr7Q6",
        "colab_type": "code",
        "outputId": "bfe7f919-6550-4709-bf30-5a2e3d151432",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "temp_test = files.upload()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-df98cd26-b175-4e95-a70f-56866877b7ac\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-df98cd26-b175-4e95-a70f-56866877b7ac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Task1C.mdl to Task1C (2).mdl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ytq8LDPX8HWh",
        "colab_type": "code",
        "outputId": "ef25d0a9-b941-401b-c0d4-d1d3bcf31a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "model = torch.load(io.BytesIO(temp_test['Task1C.mdl']))\n",
        "print(model)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task1(\n",
            "  (lin1): Linear(in_features=32048, out_features=200, bias=True)\n",
            "  (lin2): Linear(in_features=200, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AOKxmKXIb-YK",
        "colab_type": "code",
        "outputId": "20851709-50b1-4d95-b290-64ab1dfc485e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "print('--- AFTER TRAINING ---')\n",
        "count = 0\n",
        "classify = 0\n",
        "tp = fp = tn = fn = 0\n",
        "for index, instance in enumerate(test):\n",
        "    logprobs = model(instance)\n",
        "    #print(logprobs)\n",
        "    pred = np.argmax(logprobs.cpu().data.numpy())\n",
        "    actual = label_to_test_tensor[index][0]\n",
        "    #print('prediction: {}'.format(pred))\n",
        "    #print('actual: {}'.format(actual))\n",
        "    \n",
        "    if(pred == 1 and actual == 1):\n",
        "      tp += 1\n",
        "    elif(pred == 1 and actual == 0):\n",
        "      fp += 1\n",
        "    elif(pred == 0 and actual == 1):\n",
        "      fn += 1\n",
        "    elif(pred == 0 and actual == 0):\n",
        "      tn += 1\n",
        "\n",
        "precision = float(tp / (tp + fp))\n",
        "recall = float(tp / (tp + fn))\n",
        "f_score = float((2 * precision * recall) / (precision + recall))\n",
        "accuracy = float((tp + tn) / (tp + tn + fp + fn))\n",
        "\n",
        "print(' precision is: ' +str(precision)+' recall is: ' +str(recall)+' F-score is: ' +str(f_score)+ ' accuracy is: '+str(accuracy))\n",
        "      "
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " precision is: 0.8684319404220108 recall is: 0.8335980937251787 F-score is: 0.8506585612968592 accuracy is: 0.8526\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}