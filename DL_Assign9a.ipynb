{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assign9a.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prach1/CS69002_9A_18CS60R30/blob/master/DL_Assign9a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WWVo8Rx4IJri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TmKKOcUJIaV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S5R4nGhQGdmT",
        "colab_type": "code",
        "outputId": "d507c412-234c-4c5b-e818-afc1a0223092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mEzZuPgfMpjC",
        "colab_type": "code",
        "outputId": "e7d39717-d0fc-43eb-c191-0f05ccb9d0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/Train_20K.csv', sep='\\t')\n",
        "print(df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  label\n",
            "0  John Waters has given us a genuinely enjoyable...      1\n",
            "1  This first two seasons of this comedy series w...      1\n",
            "2  What an unfortunate mess is \"Shiner.\" I wanted...      0\n",
            "3  I'm not entirely sure Rob Schmidt qualifies as...      1\n",
            "4  i wasn't sure whether to laugh or cry. Porrett...      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OgkNiKQHP7zY",
        "colab_type": "code",
        "outputId": "56843b14-932f-4e5b-8904-e9f10b71f616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/Test_5K.csv', sep='\\t')\n",
        "print(df1.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  label\n",
            "0  as always this is an inaccurate picture of the...      0\n",
            "1  Did the movie-makers even preview this before ...      0\n",
            "2  Heavily re-edited and often confusing, the ori...      0\n",
            "3  I notice that most of the people who think thi...      0\n",
            "4  First of all, this is a low-budget movie, so m...      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oOFOygSRGxYf",
        "colab_type": "code",
        "outputId": "c5f5eaad-bb4b-4d1f-9b3c-b62ee8a87557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/Final_Test.csv', header = None, sep='\\n')\n",
        "print(df2.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   0\n",
            "0  CAUTION: SPOILERS<br /><br />Although this fil...\n",
            "1  I generally like this movie a lot. The animati...\n",
            "2  I don't see the sense in going through so much...\n",
            "3  This movie is an abortive, stillborn attempt t...\n",
            "4  It's amazing that this no talent actor Chapa g...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fUNCUbMVNmed",
        "colab_type": "code",
        "outputId": "635a1a52-378a-4034-d1eb-296750c868ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "for chunk in df2:\n",
        "  print(chunk)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XNFMU4oAPObi",
        "colab_type": "code",
        "outputId": "8641cb09-282a-46b7-a966-fd981f90ecdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# for i, j in df.iterrows(): \n",
        "#     print(i)\n",
        "#     print(j)\n",
        "\n",
        "print(type(df['text']))\n",
        "print(type(df['label']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKNDD3sbP-ou",
        "colab_type": "code",
        "outputId": "51498908-bf18-4a19-abda-52d670e01230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text_reviews = df['text'].astype(str).tolist()\n",
        "text_labels = df['label'].astype(int)\n",
        "print(type(text_labels))\n",
        "\n",
        "text_reviews = [x.lower() for x in text_reviews]\n",
        "#print(type(text_reviews))\n",
        "#print(text_reviews[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mV2qpcyVQGPc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_test_reviews = df1['text'].astype(str).tolist()\n",
        "text_test_labels = df1['label'].astype(int)\n",
        "#print(type(text_labels))\n",
        "\n",
        "text_test_reviews = [x.lower() for x in text_test_reviews]\n",
        "#print(type(text_reviews))\n",
        "#print(text_reviews[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_IgaxY_J1ev",
        "colab_type": "code",
        "outputId": "f4a0ff45-94f0-40aa-ac3f-5c3dc6937593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "pred_reviews = df2[0].astype(str).tolist()\n",
        "print(pred_reviews[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CAUTION: SPOILERS<br /><br />Although this film moved a bit slow at times, the brilliant scenery, richness of the characters and powerful themes make `Morte a Venezia' a rewarding experience. I have not read Thomas Mann's book, but I am certain that Visconti's visual splendour, musical score, and powerful evocation of conflict and desire must do it justice. <br /><br />The study of Gustav von Aschenbach alludes to the human tendency to rationalize and quantify our emotions, behaviour and passion. This tendency is demonstrated in the scene in Germany between Alfred and Gustav when Alfred describes Music as being both mathematical--i.e. quantifiable--and emotional. This conflict arises again in the scene where young Tadzio is alone playing `Fuer Elise' in lobby of the Hotel and Gustav recalls his visit to a bordello where he is drawn to a prostitute who plays the same song. In his flashback, after paying the prostitute, Gustav is clearly physically seized by the consequences of his actions. This reaction acts as a reminder of the moral reaction to the temptations that Tadzio represents.<br /><br />Ultimately, Gustav is forced to make his biggest decision: stay in Venice and resign himself to his lust and temptations? Or flee Venice to save his own life? His early attempt to flee Venice at the train station resulted in a futility and foreshadows the outcome of prolonging his stay.<br /><br />Complimenting the captivating character interaction, Visconti's powerful scenery (especially of Venice at Dawn and the final scene of Tadzio walking into the water and pointing to the horizon) renders this film a true masterpiece.\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5YSFe6HCSDIc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess(text):    \n",
        "  text = re.sub(r'<br.*?>',' ',text)\n",
        "  text = re.sub(r'[^\\w\\s]',' ',text) \n",
        "  text = re.sub(r'[0-9]+', ' ', text)\n",
        "  \n",
        "  return text\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLmTNitJKXqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training Data Pre-processed**"
      ]
    },
    {
      "metadata": {
        "id": "EasTRcAhRolZ",
        "colab_type": "code",
        "outputId": "d6a5e2f5-8dad-4208-e7fa-0ef16d684c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "train_data = []\n",
        "train_data_X = []\n",
        "stop = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for review in text_reviews:\n",
        "  new_review = preprocess(review)\n",
        "  train_data.append(new_review)\n",
        "  \n",
        "print(train_data[0])\n",
        "\n",
        "for review in train_data:\n",
        "  review = [lemmatizer.lemmatize(word) for word in review.split() if word not in stop and word != '' and len(lemmatizer.lemmatize(word)) > 1]\n",
        "  train_data_X.append(review)\n",
        "  \n",
        "print(train_data_X[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "john waters has given us a genuinely enjoyable film  this certainly isn t without its shocking waters esque moments  but it is tamer than his older culty stuff  such as  pink flamingoes    pecker  harkens back to john s early mainstream stage in that it reminds the viewer of the same kind of humor that was evident in  polyester   overall  a really fun comedy with some great moments \n",
            "['john', 'water', 'given', 'genuinely', 'enjoyable', 'film', 'certainly', 'without', 'shocking', 'water', 'esque', 'moment', 'tamer', 'older', 'culty', 'stuff', 'pink', 'flamingo', 'pecker', 'harkens', 'back', 'john', 'early', 'mainstream', 'stage', 'reminds', 'viewer', 'kind', 'humor', 'evident', 'polyester', 'overall', 'really', 'fun', 'comedy', 'great', 'moment']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FqFW0qSMKhrr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Testing Data Pre-processed**"
      ]
    },
    {
      "metadata": {
        "id": "7Zt6iRhMQRxv",
        "colab_type": "code",
        "outputId": "0aa516fe-20d2-4013-b313-b8740b7f8632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "test_data = []\n",
        "test_data_X = []\n",
        "stop = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for review in text_test_reviews:\n",
        "  new_review = preprocess(review)\n",
        "  test_data.append(new_review)\n",
        "  \n",
        "print(test_data[0])\n",
        "\n",
        "for review in test_data:\n",
        "  review = [lemmatizer.lemmatize(word) for word in review.split() if word not in stop and word != '' and len(lemmatizer.lemmatize(word)) > 1]\n",
        "  test_data_X.append(review)\n",
        "  \n",
        "print(test_data_X[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "as always this is an inaccurate picture of the homeless  tv told a lot of lies about panhandlers in the early  s and made everyone look bad  and claimed we all made over    a day when      a day was much closer to reality  when someone drove by where i held up a sign offering to work  and offered me work  i actually went and took the work if i was physically able and if i would been offered the      id damned sure invested in in apt prepaid for at least   years  and kept most in the bank and still left myself      for nl      and      cash games at the casinos  i usually always win and could win decent if i just had a bankroll  instead i win about    a month is all playing in always minimum buying in due to not wanting to risk losing it all  i was only homeless cause i didn t wanna risk spending all my money and going broke  sometimes i had over      in my sock while i slept outside  anyone wanting to talk contact sevencard  on yahoo messenger i admit i was different than most homeless people though  due to the fact i never drank smoke or took drugs  im no longer homeless  am now in govt housing for    a month and getting ssi and spend most of my time winning at online poker  mom and sunflower diversified worked hard to get me ssi  glad my days of hiding in under the stage in the convention center of the casino at night sleeping  worrying about getting caught by security are finally over  had this tv crew picked me theyd been over a lot sooner  its a shame how they don t better select who they pick \n",
            "['always', 'inaccurate', 'picture', 'homeless', 'tv', 'told', 'lot', 'lie', 'panhandler', 'early', 'made', 'everyone', 'look', 'bad', 'claimed', 'made', 'day', 'day', 'much', 'closer', 'reality', 'someone', 'drove', 'held', 'sign', 'offering', 'work', 'offered', 'work', 'actually', 'went', 'took', 'work', 'physically', 'able', 'would', 'offered', 'id', 'damned', 'sure', 'invested', 'apt', 'prepaid', 'least', 'year', 'kept', 'bank', 'still', 'left', 'nl', 'cash', 'game', 'casino', 'usually', 'always', 'win', 'could', 'win', 'decent', 'bankroll', 'instead', 'win', 'month', 'playing', 'always', 'minimum', 'buying', 'due', 'wanting', 'risk', 'losing', 'homeless', 'cause', 'wanna', 'risk', 'spending', 'money', 'going', 'broke', 'sometimes', 'sock', 'slept', 'outside', 'anyone', 'wanting', 'talk', 'contact', 'sevencard', 'yahoo', 'messenger', 'admit', 'different', 'homeless', 'people', 'though', 'due', 'fact', 'never', 'drank', 'smoke', 'took', 'drug', 'im', 'longer', 'homeless', 'govt', 'housing', 'month', 'getting', 'ssi', 'spend', 'time', 'winning', 'online', 'poker', 'mom', 'sunflower', 'diversified', 'worked', 'hard', 'get', 'ssi', 'glad', 'day', 'hiding', 'stage', 'convention', 'center', 'casino', 'night', 'sleeping', 'worrying', 'getting', 'caught', 'security', 'finally', 'tv', 'crew', 'picked', 'theyd', 'lot', 'sooner', 'shame', 'better', 'select', 'pick']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wnMm6CM6KlM7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Prediction Data Pre-processed**"
      ]
    },
    {
      "metadata": {
        "id": "BZp_CUVwKMvF",
        "colab_type": "code",
        "outputId": "de09c241-25b3-49d3-9ed9-9e70e31fc5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "pred_data = []\n",
        "pred_data_X = []\n",
        "stop = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for review in pred_reviews:\n",
        "  new_review = preprocess(review)\n",
        "  pred_data.append(new_review)\n",
        "  \n",
        "print(pred_data[0])\n",
        "\n",
        "for review in pred_data:\n",
        "  review = [lemmatizer.lemmatize(word) for word in review.split() if word not in stop and word != '' and len(lemmatizer.lemmatize(word)) > 1]\n",
        "  pred_data_X.append(review)\n",
        "  \n",
        "print(pred_data_X[0])\n",
        "print(len(pred_data_X))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CAUTION  SPOILERS  Although this film moved a bit slow at times  the brilliant scenery  richness of the characters and powerful themes make  Morte a Venezia  a rewarding experience  I have not read Thomas Mann s book  but I am certain that Visconti s visual splendour  musical score  and powerful evocation of conflict and desire must do it justice    The study of Gustav von Aschenbach alludes to the human tendency to rationalize and quantify our emotions  behaviour and passion  This tendency is demonstrated in the scene in Germany between Alfred and Gustav when Alfred describes Music as being both mathematical  i e  quantifiable  and emotional  This conflict arises again in the scene where young Tadzio is alone playing  Fuer Elise  in lobby of the Hotel and Gustav recalls his visit to a bordello where he is drawn to a prostitute who plays the same song  In his flashback  after paying the prostitute  Gustav is clearly physically seized by the consequences of his actions  This reaction acts as a reminder of the moral reaction to the temptations that Tadzio represents   Ultimately  Gustav is forced to make his biggest decision  stay in Venice and resign himself to his lust and temptations  Or flee Venice to save his own life  His early attempt to flee Venice at the train station resulted in a futility and foreshadows the outcome of prolonging his stay   Complimenting the captivating character interaction  Visconti s powerful scenery  especially of Venice at Dawn and the final scene of Tadzio walking into the water and pointing to the horizon  renders this film a true masterpiece \t\n",
            "['CAUTION', 'SPOILERS', 'Although', 'film', 'moved', 'bit', 'slow', 'time', 'brilliant', 'scenery', 'richness', 'character', 'powerful', 'theme', 'make', 'Morte', 'Venezia', 'rewarding', 'experience', 'read', 'Thomas', 'Mann', 'book', 'certain', 'Visconti', 'visual', 'splendour', 'musical', 'score', 'powerful', 'evocation', 'conflict', 'desire', 'must', 'justice', 'The', 'study', 'Gustav', 'von', 'Aschenbach', 'alludes', 'human', 'tendency', 'rationalize', 'quantify', 'emotion', 'behaviour', 'passion', 'This', 'tendency', 'demonstrated', 'scene', 'Germany', 'Alfred', 'Gustav', 'Alfred', 'describes', 'Music', 'mathematical', 'quantifiable', 'emotional', 'This', 'conflict', 'arises', 'scene', 'young', 'Tadzio', 'alone', 'playing', 'Fuer', 'Elise', 'lobby', 'Hotel', 'Gustav', 'recall', 'visit', 'bordello', 'drawn', 'prostitute', 'play', 'song', 'In', 'flashback', 'paying', 'prostitute', 'Gustav', 'clearly', 'physically', 'seized', 'consequence', 'action', 'This', 'reaction', 'act', 'reminder', 'moral', 'reaction', 'temptation', 'Tadzio', 'represents', 'Ultimately', 'Gustav', 'forced', 'make', 'biggest', 'decision', 'stay', 'Venice', 'resign', 'lust', 'temptation', 'Or', 'flee', 'Venice', 'save', 'life', 'His', 'early', 'attempt', 'flee', 'Venice', 'train', 'station', 'resulted', 'futility', 'foreshadows', 'outcome', 'prolonging', 'stay', 'Complimenting', 'captivating', 'character', 'interaction', 'Visconti', 'powerful', 'scenery', 'especially', 'Venice', 'Dawn', 'final', 'scene', 'Tadzio', 'walking', 'water', 'pointing', 'horizon', 'render', 'film', 'true', 'masterpiece']\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-h1LFhDrcFgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_word_frequency(dataset):\n",
        "  word_to_frequency = {}\n",
        " \n",
        "  for sent in dataset:\n",
        "    for word in sent:\n",
        "      if word not in word_to_frequency:\n",
        "        word_to_frequency[word] = 1\n",
        "      else:\n",
        "        word_to_frequency[word] += 1\n",
        "              \n",
        "  return word_to_frequency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gP8ClEwsGRHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_bigram_word_frequency(dataset):\n",
        "  word_to_frequency = {}\n",
        "  \n",
        "  for sent in dataset:\n",
        "    for index,word1 in enumerate(sent[:-1]):\n",
        "      word2 = sent[index+1]\n",
        "      if (word1,word2) not in word_to_frequency:\n",
        "        word_to_frequency[(word1,word2)] = 1\n",
        "      else:\n",
        "        word_to_frequency[(word1,word2)] += 1\n",
        "              \n",
        "  return word_to_frequency  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jT64fOuh1bRE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_word_ids(dataset):\n",
        "  word_to_ix = {}\n",
        " \n",
        "  for word in dataset:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "              \n",
        "  word_to_ix['UNKNOWN'] = len(word_to_ix) \n",
        "              \n",
        "  return word_to_ix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDk2HppkLNmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_bigram_word_ids(dataset):\n",
        "  word_to_ix = {}\n",
        " \n",
        "  for word in dataset:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "              \n",
        "  word_to_ix['UNKNOWN'] = len(word_to_ix) \n",
        "              \n",
        "  return word_to_ix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBuHYkyVbwj_",
        "colab_type": "code",
        "outputId": "b6062bd1-2e34-478f-de98-1b90fd36988f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "word_to_frequency = generate_word_frequency(train_data_X + test_data_X)\n",
        "# for key in word_to_bigram_frequency:\n",
        "#   print(key)\n",
        "#   break\n",
        "\n",
        "new_word_dataset = []\n",
        "\n",
        "for word in word_to_frequency:\n",
        "  if(word_to_frequency[word] >= 3):\n",
        "    new_word_dataset.append(word)\n",
        "        \n",
        "print(len(word_to_frequency))\n",
        "print(len(new_word_dataset))\n",
        "\n",
        "word_to_ix = generate_word_ids(new_word_dataset)\n",
        "\n",
        "print(len(word_to_ix))\n",
        "print(word_to_ix['UNKNOWN'])\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = 2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63726\n",
            "32047\n",
            "32048\n",
            "32047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9J59Wdd_Ypjr",
        "colab_type": "code",
        "outputId": "64d63fc0-d3ca-4c6c-d987-8101fa006cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "word_to_bigram_frequency = generate_bigram_word_frequency(train_data_X + test_data_X)\n",
        "new_bigram_word_dataset = []\n",
        "\n",
        "for word in word_to_bigram_frequency:\n",
        "  if(word_to_bigram_frequency[word] >= 5):\n",
        "    new_bigram_word_dataset.append(word)\n",
        "    \n",
        "print(len(word_to_bigram_frequency))\n",
        "print(len(new_bigram_word_dataset))\n",
        "#print(new_bigram_word_dataset[0])\n",
        "\n",
        "word_to_bigram_ix = generate_bigram_word_ids(new_bigram_word_dataset)\n",
        "BIGRAM_VOCAB_SIZE = len(word_to_bigram_ix)\n",
        "print(BIGRAM_VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1554114\n",
            "63711\n",
            "63712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kCBikQgFeNzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bow_vector(sentence):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            #raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            vec[word_to_ix['UNKNOWN']]+=1\n",
        "        else:\n",
        "            vec[word_to_ix[word]]+=1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "# def make_target(label, label_to_ix):\n",
        "#     return torch.LongTensor([label_to_ix[label]])\n",
        "    #return torch.LongTensor(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVol8s9ZOe1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bigram_bow_vector(sentence):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_bigram_ix))\n",
        "    for index,word1 in enumerate(sentence[:-1]):\n",
        "      word2 = sentence[index+1]      \n",
        "      if (word1,word2) in word_to_bigram_ix:\n",
        "        vec[word_to_bigram_ix[(word1,word2)]]+=1\n",
        "#         vec[word_to_bigram_ix['UNKNOWN']]+=1\n",
        "#       if (word1,word2) not in word_to_bigram_ix:\n",
        "#         vec[word_to_bigram_ix['UNKNOWN']]+=1\n",
        "#       else:\n",
        "#         vec[word_to_bigram_ix[(word1,word2)]]+=1\n",
        "    return vec.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eVHXo-z6o5Ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = []\n",
        "\n",
        "for review in train_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bow_vector(review)).cuda()\n",
        "  train.append(embedding)\n",
        "  \n",
        "train = torch.stack(train).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gObiNRC_yUF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4daeTqFXOisC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bigram_train = []\n",
        "\n",
        "for review in train_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bigram_bow_vector(review))\n",
        "  bigram_train.append(embedding)\n",
        "\n",
        "bigram_train = torch.stack(bigram_train).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lO7x_vpokwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "\n",
        "test = []\n",
        "for review in test_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bow_vector(review)).cuda()\n",
        "  test.append(embedding)\n",
        "  \n",
        "test= torch.stack(test).cuda()\n",
        "  \n",
        "# fp = open(\"test.pickle\",\"wb\")\n",
        "# pickle.dump(test, fp)\n",
        "# fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLGnoEvOOlsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bigram_test = []\n",
        "for review in test_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bigram_bow_vector(review)).cuda()\n",
        "  bigram_test.append(embedding)\n",
        "  \n",
        "bigram_test = torch.stack(bigram_test).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hB2DOyTwLEI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict = []\n",
        "\n",
        "for review in pred_data_X:\n",
        "  embedding = torch.autograd.Variable(make_bow_vector(review)).cuda()\n",
        "  predict.append(embedding)\n",
        "  \n",
        "predict = torch.stack(predict).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q629yqi92ICc",
        "colab_type": "code",
        "outputId": "419da647-9efa-4f78-888a-581042a674f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(train[0])\n",
        "print(len(train))\n",
        "print(test[4])\n",
        "print(len(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 1.,  ..., 0., 0., 1.]], device='cuda:0')\n",
            "17999\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uUMS1uUm5agi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data = []\n",
        "\n",
        "# for index,review in enumerate(train):\n",
        "#   data.append((review,text_labels[index]))\n",
        "  \n",
        "# print(data[0])\n",
        "\n",
        "#label_to_tensor = torch.LongTensor(text_labels)\n",
        "\n",
        "label_to_tensor = []\n",
        "\n",
        "#print(text_labels)\n",
        "text_labels1 = []\n",
        "\n",
        "for label in text_labels:\n",
        "  text_labels1.append([label])\n",
        "  \n",
        "\n",
        "#print(text_labels1)\n",
        "\n",
        "for label in text_labels1:\n",
        "  label_to_tensor.append(torch.tensor(label, dtype = torch.long))\n",
        "  \n",
        "label_to_tensor = torch.stack(label_to_tensor).cuda()\n",
        "  #print(label_to_tensor)\n",
        "  \n",
        "#print(label_to_tensor)\n",
        "\n",
        "#for label in label_to_tensor:\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TAlc4HZIuuSM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_to_test_tensor = []\n",
        "\n",
        "text_test_labels1 = []\n",
        "\n",
        "for label in text_test_labels:\n",
        "  text_test_labels1.append([label])\n",
        "\n",
        "for label in text_test_labels1:\n",
        "  label_to_test_tensor.append(torch.tensor(label, dtype = torch.long))\n",
        "  \n",
        "label_to_test_tensor = torch.stack(label_to_test_tensor).cuda()\n",
        "  \n",
        "#print(label_to_test_tensor)\n",
        "# fp = open(\"test.pickle\",\"wb\")\n",
        "# pickle.dump(label_to_test_tensor, fp)\n",
        "# fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLRwTztI8VPb",
        "colab_type": "code",
        "outputId": "258a7b61-f397-4871-fffe-2b94f3ace0a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.device('cuda') "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "3X4ztERz78r1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task1(nn.Module):\n",
        "  def __init__(self, num_labels,hidden,vocab_size):\n",
        "    super(Task1, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden)\n",
        "    self.lin2 = nn.Linear(hidden, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.softmax(self.lin2(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtj4mNPeuMes",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task2(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,vocab_size):\n",
        "    super(Task2, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    self.lin3 = nn.Linear(hidden2, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = F.softmax(self.lin3(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mYePz-uhxAKJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task3(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,hidden3,vocab_size):\n",
        "    super(Task3, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    self.lin3 = nn.Linear(hidden2, hidden3)\n",
        "    self.lin4 = nn.Linear(hidden3, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    out = F.softmax(self.lin4(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xf1EUT_b1anV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task4(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,vocab_size):\n",
        "    super(Task4, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin3 = nn.Linear(hidden2, NUM_LABELS)\n",
        "#     self.lin3 = nn.Linear(hidden2, hidden3)\n",
        "#     torch.nn.Dropout(0.5)\n",
        "#     self.lin4 = nn.Linear(hidden3, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    #out = self.lin3(out)\n",
        "    out = F.softmax(self.lin3(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkpLnBaCgRt4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task5(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,hidden3,vocab_size):\n",
        "    super(Task5, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin3 = nn.Linear(hidden2, hidden3)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin4 = nn.Linear(hidden3, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.sigmoid(self.lin2(out))\n",
        "    out = F.sigmoid(self.lin3(out))\n",
        "    out = F.softmax(self.lin4(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RaIw3shQU3TN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task6(nn.Module):\n",
        "  def __init__(self, num_labels,hidden1,hidden2,hidden3,vocab_size):\n",
        "    super(Task6, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden1)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin2 = nn.Linear(hidden1, hidden2)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin3 = nn.Linear(hidden2, hidden3)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin4 = nn.Linear(hidden3, NUM_LABELS)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    out = F.softmax(self.lin4(out))\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gsz5Y1JhXaKy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Object Creation"
      ]
    },
    {
      "metadata": {
        "id": "iGsLyVkVVOrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TASK 1**"
      ]
    },
    {
      "metadata": {
        "id": "EYmdKRzz8k0H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "o0RdLlbP8Gv9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden = 500\n",
        "bow = Task1(NUM_LABELS,hidden,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r6Du5aROVR-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ]
    },
    {
      "metadata": {
        "id": "OM82SJiyu-RY",
        "colab_type": "code",
        "outputId": "1522f655-7a6d-4f64-8256-136a8e8fd3e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 100\n",
        "hidden2 = 50\n",
        "bow = Task2(NUM_LABELS,hidden1,hidden2,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task2(\n",
            "  (lin1): Linear(in_features=32048, out_features=100, bias=True)\n",
            "  (lin2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (lin3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PVPsBYp8VUAb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TASK 3**"
      ]
    },
    {
      "metadata": {
        "id": "GLf_5IEgxVHY",
        "colab_type": "code",
        "outputId": "7ef57719-92f1-4ff6-d7c3-f32440ac8eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 200\n",
        "hidden2 = 100\n",
        "hidden3 = 10\n",
        "bow = Task3(NUM_LABELS,hidden1,hidden2,hidden3,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task3(\n",
            "  (lin1): Linear(in_features=32048, out_features=200, bias=True)\n",
            "  (lin2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (lin3): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (lin4): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G97EgKLcVWZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TASK 4**"
      ]
    },
    {
      "metadata": {
        "id": "3QZcU4yR3xzD",
        "colab_type": "code",
        "outputId": "72081493-3391-4144-9b3f-0e4a304c450a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 100\n",
        "hidden2 = 100\n",
        "#hidden3 = 10\n",
        "bow = Task4(NUM_LABELS,hidden1,hidden2,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task4(\n",
            "  (lin1): Linear(in_features=32048, out_features=100, bias=True)\n",
            "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (lin3): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iwyyW-Q3gET5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Task 5**"
      ]
    },
    {
      "metadata": {
        "id": "QxrMwfAxgGTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "dc9029ae-1c9b-4c27-d6fb-f3d37fd596c2"
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 30\n",
        "hidden2 = 20\n",
        "hidden3 = 10\n",
        "bow = Task5(NUM_LABELS,hidden1,hidden2,hidden3,VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task5(\n",
            "  (lin1): Linear(in_features=32048, out_features=30, bias=True)\n",
            "  (lin2): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (lin3): Linear(in_features=20, out_features=10, bias=True)\n",
            "  (lin4): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wl2OeEHBgBHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Task 6**"
      ]
    },
    {
      "metadata": {
        "id": "SONV3i30U9ej",
        "colab_type": "code",
        "outputId": "21c3b634-f1f4-4a29-f730-47af9e847c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "hidden1 = 30\n",
        "hidden2 = 20\n",
        "hidden3 = 10\n",
        "bow = Task6(NUM_LABELS,hidden1,hidden2,hidden3,BIGRAM_VOCAB_SIZE).cuda()\n",
        "print(bow)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task6(\n",
            "  (lin1): Linear(in_features=63712, out_features=30, bias=True)\n",
            "  (lin2): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (lin3): Linear(in_features=20, out_features=10, bias=True)\n",
            "  (lin4): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8E_6XAY8o56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "#loss_function = nn.MSELoss()\n",
        "#loss_function = nn.HingeEmbeddingLoss()\n",
        "#loss_function = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(bow.parameters(), lr = 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lv27PyHmTJtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ]
    },
    {
      "metadata": {
        "id": "IyjEzEvo-BIl",
        "colab_type": "code",
        "outputId": "5b29f57f-d329-482d-bd04-ce549faf21b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# the training loop\n",
        "for epoch in range(10):\n",
        "  start = time.time()\n",
        "  for index, instance in enumerate(train):\n",
        "    bow.zero_grad()\n",
        "    label = Variable(label_to_tensor[index]).cuda()       \n",
        "    probs = bow(instance).cuda() # forward pass\n",
        "    loss = loss_function(probs, label)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  end = time.time()  \n",
        "  print('epoch is: ' +str(epoch) + ' and loss is: ' +str(loss.data) + ' and time taken is: ' +str(end-start))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch is: 0 and loss is: tensor(-0.3687, device='cuda:0') and time taken is: 25.27102780342102\n",
            "epoch is: 1 and loss is: tensor(-0.9936, device='cuda:0') and time taken is: 25.7508282661438\n",
            "epoch is: 2 and loss is: tensor(-0.9993, device='cuda:0') and time taken is: 25.08737087249756\n",
            "epoch is: 3 and loss is: tensor(-0.9996, device='cuda:0') and time taken is: 25.08511233329773\n",
            "epoch is: 4 and loss is: tensor(-0.9997, device='cuda:0') and time taken is: 25.546654224395752\n",
            "epoch is: 5 and loss is: tensor(-0.9998, device='cuda:0') and time taken is: 25.20262098312378\n",
            "epoch is: 6 and loss is: tensor(-0.9999, device='cuda:0') and time taken is: 25.116793632507324\n",
            "epoch is: 7 and loss is: tensor(-0.9999, device='cuda:0') and time taken is: 25.09343409538269\n",
            "epoch is: 8 and loss is: tensor(-0.9999, device='cuda:0') and time taken is: 25.11008858680725\n",
            "epoch is: 9 and loss is: tensor(-0.9999, device='cuda:0') and time taken is: 25.06444239616394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BI41vCPuTNBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Bigram Training**"
      ]
    },
    {
      "metadata": {
        "id": "o_YBrIQcVL1A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Bigram#\n",
        "\n",
        "import time\n",
        "\n",
        "# the training loop\n",
        "for epoch in range(10):\n",
        "  start = time.time()\n",
        "  for index, instance in enumerate(bigram_train):\n",
        "    bow.zero_grad()\n",
        "    label = Variable(label_to_tensor[index]).cuda()       \n",
        "    probs = bow(instance).cuda() # forward pass\n",
        "    loss = loss_function(probs, label)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "  end = time.time()  \n",
        "  print('epoch is: ' +str(epoch) + ' and loss is: ' +str(loss.data) + ' and time taken is: ' +str(end-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1XUHc0VVjNe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Save The Model**"
      ]
    },
    {
      "metadata": {
        "id": "TFTWGa_wre-1",
        "colab_type": "code",
        "outputId": "0528e3de-8c30-40f1-dee2-40054c779656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "torch.save(bow.cpu(),'Task5C.mdl')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('Task5C.mdl')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Task5. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "E1Iyc2KHr7Q6",
        "colab_type": "code",
        "outputId": "71c3376e-788f-4112-b735-12aa1ed0a77b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "temp_test = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-432c484e-0781-4d60-bcbb-af80c7b8e68b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-432c484e-0781-4d60-bcbb-af80c7b8e68b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Task6 (4).mdl to Task6 (4).mdl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EaOZysINsLHt",
        "colab_type": "code",
        "outputId": "a0f53982-1a59-40e6-8f4b-699b7d49d204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(type(temp_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ytq8LDPX8HWh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = torch.load(io.BytesIO(temp_test['Task1.mdl']))\n",
        "# checkpoint = torch.load('Task1D.mdl')\n",
        "# model = checkpoint['model']\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "# print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WH9UGGGoVmxe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Testing Time**"
      ]
    },
    {
      "metadata": {
        "id": "AOKxmKXIb-YK",
        "colab_type": "code",
        "outputId": "dc39281d-f880-484d-900b-0bdb89d51436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "print('--- AFTER TRAINING ---')\n",
        "count = 0\n",
        "classify = 0\n",
        "tp = fp = tn = fn = 0\n",
        "for index, instance in enumerate(test.cpu()):\n",
        "    logprobs = bow(instance)\n",
        "    #print(logprobs)\n",
        "    pred = np.argmax(logprobs.cpu().data.numpy())\n",
        "    actual = label_to_test_tensor[index][0]\n",
        "    #print('prediction: {}'.format(pred))\n",
        "    #print('actual: {}'.format(actual))\n",
        "    \n",
        "    if(pred == 1 and actual == 1):\n",
        "      tp += 1\n",
        "    elif(pred == 1 and actual == 0):\n",
        "      fp += 1\n",
        "    elif(pred == 0 and actual == 1):\n",
        "      fn += 1\n",
        "    elif(pred == 0 and actual == 0):\n",
        "      tn += 1\n",
        "\n",
        "precision = float(tp / (tp + fp))\n",
        "recall = float(tp / (tp + fn))\n",
        "f_score = float((2 * precision * recall) / (precision + recall))\n",
        "accuracy = float((tp + tn) / (tp + tn + fp + fn))\n",
        "\n",
        "print(' precision is: ' +str(precision)+' recall is: ' +str(recall)+' F-score is: ' +str(f_score)+ ' accuracy is: '+str(accuracy))\n",
        "      "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " precision is: 0.8777416734362307 recall is: 0.8582208101667991 F-score is: 0.8678714859437751 accuracy is: 0.8684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_4V_kaeoVqLE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Predictions**"
      ]
    },
    {
      "metadata": {
        "id": "eRORtx7xEEV0",
        "colab_type": "code",
        "outputId": "e7a507e6-f852-49ad-9cdb-3afafb9cee4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "fp = open('pred_task5C.csv','a')\n",
        "print('--- AFTER TESTING ---')\n",
        "\n",
        "for index, instance in enumerate(predict.cpu()):\n",
        "    logprobs = bow(instance)\n",
        "    prediction = np.argmax(logprobs.cpu().data.numpy())\n",
        "    fp.write(prediction.astype(str) + '\\n')\n",
        "      "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TESTING ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}